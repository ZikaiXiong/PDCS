//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-32267302
// Cuda compilation tools, release 12.0, V12.0.140
// Based on NVVM 7.0.1
//

.version 8.0
.target sm_90
.address_size 64

	// .globl	primal_update

.visible .entry primal_update(
	.param .u64 primal_update_param_0,
	.param .u64 primal_update_param_1,
	.param .u64 primal_update_param_2,
	.param .u64 primal_update_param_3,
	.param .f64 primal_update_param_4,
	.param .u64 primal_update_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [primal_update_param_0];
	ld.param.u64 	%rd3, [primal_update_param_1];
	ld.param.u64 	%rd4, [primal_update_param_2];
	ld.param.u64 	%rd5, [primal_update_param_3];
	ld.param.f64 	%fd1, [primal_update_param_4];
	ld.param.u64 	%rd6, [primal_update_param_5];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd6;
	@%p1 bra 	$L__BB0_2;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd4;
	add.s64 	%rd11, %rd10, %rd8;
	cvta.to.global.u64 	%rd12, %rd5;
	add.s64 	%rd13, %rd12, %rd8;
	ld.global.f64 	%fd2, [%rd13];
	ld.global.f64 	%fd3, [%rd11];
	sub.f64 	%fd4, %fd3, %fd2;
	ld.global.f64 	%fd5, [%rd9];
	fma.rn.f64 	%fd6, %fd4, %fd1, %fd5;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd8;
	st.global.f64 	[%rd15], %fd6;

$L__BB0_2:
	ret;

}
	// .globl	extrapolation_update
.visible .entry extrapolation_update(
	.param .u64 extrapolation_update_param_0,
	.param .u64 extrapolation_update_param_1,
	.param .u64 extrapolation_update_param_2,
	.param .u64 extrapolation_update_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [extrapolation_update_param_0];
	ld.param.u64 	%rd3, [extrapolation_update_param_1];
	ld.param.u64 	%rd4, [extrapolation_update_param_2];
	ld.param.u64 	%rd5, [extrapolation_update_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd5;
	@%p1 bra 	$L__BB1_2;

	cvta.to.global.u64 	%rd6, %rd3;
	shl.b64 	%rd7, %rd1, 3;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	add.f64 	%fd2, %fd1, %fd1;
	cvta.to.global.u64 	%rd9, %rd4;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f64 	%fd3, [%rd10];
	sub.f64 	%fd4, %fd2, %fd3;
	cvta.to.global.u64 	%rd11, %rd2;
	add.s64 	%rd12, %rd11, %rd7;
	st.global.f64 	[%rd12], %fd4;

$L__BB1_2:
	ret;

}
	// .globl	dual_update
.visible .entry dual_update(
	.param .u64 dual_update_param_0,
	.param .u64 dual_update_param_1,
	.param .u64 dual_update_param_2,
	.param .u64 dual_update_param_3,
	.param .f64 dual_update_param_4,
	.param .u64 dual_update_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<8>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd2, [dual_update_param_0];
	ld.param.u64 	%rd3, [dual_update_param_1];
	ld.param.u64 	%rd4, [dual_update_param_2];
	ld.param.u64 	%rd5, [dual_update_param_3];
	ld.param.f64 	%fd1, [dual_update_param_4];
	ld.param.u64 	%rd6, [dual_update_param_5];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd6;
	@%p1 bra 	$L__BB2_2;

	cvta.to.global.u64 	%rd7, %rd3;
	shl.b64 	%rd8, %rd1, 3;
	add.s64 	%rd9, %rd7, %rd8;
	cvta.to.global.u64 	%rd10, %rd4;
	add.s64 	%rd11, %rd10, %rd8;
	cvta.to.global.u64 	%rd12, %rd5;
	add.s64 	%rd13, %rd12, %rd8;
	ld.global.f64 	%fd2, [%rd13];
	ld.global.f64 	%fd3, [%rd11];
	sub.f64 	%fd4, %fd3, %fd2;
	mul.f64 	%fd5, %fd4, %fd1;
	ld.global.f64 	%fd6, [%rd9];
	sub.f64 	%fd7, %fd6, %fd5;
	cvta.to.global.u64 	%rd14, %rd2;
	add.s64 	%rd15, %rd14, %rd8;
	st.global.f64 	[%rd15], %fd7;

$L__BB2_2:
	ret;

}
	// .globl	reflection_update
.visible .entry reflection_update(
	.param .u64 reflection_update_param_0,
	.param .u64 reflection_update_param_1,
	.param .u64 reflection_update_param_2,
	.param .u64 reflection_update_param_3,
	.param .u64 reflection_update_param_4,
	.param .u64 reflection_update_param_5,
	.param .f64 reflection_update_param_6,
	.param .u64 reflection_update_param_7,
	.param .u64 reflection_update_param_8,
	.param .u64 reflection_update_param_9,
	.param .f64 reflection_update_param_10,
	.param .f64 reflection_update_param_11
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<36>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd2, [reflection_update_param_0];
	ld.param.u64 	%rd3, [reflection_update_param_1];
	ld.param.u64 	%rd4, [reflection_update_param_2];
	ld.param.u64 	%rd5, [reflection_update_param_3];
	ld.param.u64 	%rd6, [reflection_update_param_4];
	ld.param.u64 	%rd7, [reflection_update_param_5];
	ld.param.f64 	%fd3, [reflection_update_param_6];
	ld.param.u64 	%rd9, [reflection_update_param_7];
	ld.param.u64 	%rd8, [reflection_update_param_8];
	ld.param.u64 	%rd10, [reflection_update_param_9];
	ld.param.f64 	%fd4, [reflection_update_param_10];
	ld.param.f64 	%fd5, [reflection_update_param_11];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	cvt.rn.f64.s64 	%fd6, %rd10;
	add.f64 	%fd7, %fd6, 0d3FF0000000000000;
	add.f64 	%fd1, %fd6, 0d4000000000000000;
	div.rn.f64 	%fd2, %fd7, %fd1;
	setp.ge.s64 	%p1, %rd1, %rd9;
	@%p1 bra 	$L__BB3_2;

	cvta.to.global.u64 	%rd11, %rd2;
	shl.b64 	%rd12, %rd1, 3;
	add.s64 	%rd13, %rd11, %rd12;
	ld.global.f64 	%fd8, [%rd13];
	add.f64 	%fd9, %fd3, 0d3FF0000000000000;
	mul.f64 	%fd10, %fd9, %fd8;
	cvta.to.global.u64 	%rd14, %rd3;
	add.s64 	%rd15, %rd14, %rd12;
	ld.global.f64 	%fd11, [%rd15];
	mul.f64 	%fd12, %fd11, %fd3;
	sub.f64 	%fd13, %fd10, %fd12;
	mul.f64 	%fd14, %fd2, %fd13;
	rcp.rn.f64 	%fd15, %fd1;
	fma.rn.f64 	%fd16, %fd15, %fd8, %fd14;
	st.global.f64 	[%rd13], %fd16;
	cvta.to.global.u64 	%rd16, %rd4;
	add.s64 	%rd17, %rd16, %rd12;
	ld.global.f64 	%fd17, [%rd17];
	mul.f64 	%fd18, %fd16, %fd5;
	fma.rn.f64 	%fd19, %fd17, %fd4, %fd18;
	add.f64 	%fd20, %fd4, %fd5;
	div.rn.f64 	%fd21, %fd19, %fd20;
	st.global.f64 	[%rd17], %fd21;

$L__BB3_2:
	setp.ge.s64 	%p2, %rd1, %rd8;
	@%p2 bra 	$L__BB3_4;

	add.f64 	%fd22, %fd3, 0d3FF0000000000000;
	cvta.to.global.u64 	%rd18, %rd5;
	shl.b64 	%rd19, %rd1, 3;
	add.s64 	%rd20, %rd18, %rd19;
	ld.global.f64 	%fd23, [%rd20];
	mul.f64 	%fd24, %fd22, %fd23;
	cvta.to.global.u64 	%rd21, %rd6;
	add.s64 	%rd22, %rd21, %rd19;
	ld.global.f64 	%fd25, [%rd22];
	mul.f64 	%fd26, %fd25, %fd3;
	sub.f64 	%fd27, %fd24, %fd26;
	mul.f64 	%fd28, %fd2, %fd27;
	rcp.rn.f64 	%fd29, %fd1;
	fma.rn.f64 	%fd30, %fd29, %fd23, %fd28;
	st.global.f64 	[%rd20], %fd30;
	cvta.to.global.u64 	%rd23, %rd7;
	add.s64 	%rd24, %rd23, %rd19;
	ld.global.f64 	%fd31, [%rd24];
	mul.f64 	%fd32, %fd30, %fd5;
	fma.rn.f64 	%fd33, %fd31, %fd4, %fd32;
	add.f64 	%fd34, %fd4, %fd5;
	div.rn.f64 	%fd35, %fd33, %fd34;
	st.global.f64 	[%rd24], %fd35;

$L__BB3_4:
	ret;

}
	// .globl	calculate_diff
.visible .entry calculate_diff(
	.param .u64 calculate_diff_param_0,
	.param .u64 calculate_diff_param_1,
	.param .u64 calculate_diff_param_2,
	.param .u64 calculate_diff_param_3,
	.param .u64 calculate_diff_param_4,
	.param .u64 calculate_diff_param_5,
	.param .u64 calculate_diff_param_6,
	.param .u64 calculate_diff_param_7
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<7>;
	.reg .b64 	%rd<24>;


	ld.param.u64 	%rd2, [calculate_diff_param_0];
	ld.param.u64 	%rd3, [calculate_diff_param_1];
	ld.param.u64 	%rd4, [calculate_diff_param_2];
	ld.param.u64 	%rd9, [calculate_diff_param_3];
	ld.param.u64 	%rd5, [calculate_diff_param_4];
	ld.param.u64 	%rd6, [calculate_diff_param_5];
	ld.param.u64 	%rd7, [calculate_diff_param_6];
	ld.param.u64 	%rd8, [calculate_diff_param_7];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd9;
	@%p1 bra 	$L__BB4_2;

	cvta.to.global.u64 	%rd10, %rd2;
	shl.b64 	%rd11, %rd1, 3;
	add.s64 	%rd12, %rd10, %rd11;
	cvta.to.global.u64 	%rd13, %rd3;
	add.s64 	%rd14, %rd13, %rd11;
	ld.global.f64 	%fd1, [%rd14];
	ld.global.f64 	%fd2, [%rd12];
	sub.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd15, %rd4;
	add.s64 	%rd16, %rd15, %rd11;
	st.global.f64 	[%rd16], %fd3;

$L__BB4_2:
	setp.ge.s64 	%p2, %rd1, %rd8;
	@%p2 bra 	$L__BB4_4;

	cvta.to.global.u64 	%rd17, %rd5;
	shl.b64 	%rd18, %rd1, 3;
	add.s64 	%rd19, %rd17, %rd18;
	cvta.to.global.u64 	%rd20, %rd6;
	add.s64 	%rd21, %rd20, %rd18;
	ld.global.f64 	%fd4, [%rd21];
	ld.global.f64 	%fd5, [%rd19];
	sub.f64 	%fd6, %fd5, %fd4;
	cvta.to.global.u64 	%rd22, %rd7;
	add.s64 	%rd23, %rd22, %rd18;
	st.global.f64 	[%rd23], %fd6;

$L__BB4_4:
	ret;

}
	// .globl	axpyz
.visible .entry axpyz(
	.param .u64 axpyz_param_0,
	.param .f64 axpyz_param_1,
	.param .u64 axpyz_param_2,
	.param .u64 axpyz_param_3,
	.param .u64 axpyz_param_4
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd2, [axpyz_param_0];
	ld.param.f64 	%fd1, [axpyz_param_1];
	ld.param.u64 	%rd3, [axpyz_param_2];
	ld.param.u64 	%rd4, [axpyz_param_3];
	ld.param.u64 	%rd5, [axpyz_param_4];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd5;
	@%p1 bra 	$L__BB5_2;

	cvta.to.global.u64 	%rd6, %rd3;
	shl.b64 	%rd7, %rd1, 3;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd2, [%rd8];
	cvta.to.global.u64 	%rd9, %rd4;
	add.s64 	%rd10, %rd9, %rd7;
	ld.global.f64 	%fd3, [%rd10];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	cvta.to.global.u64 	%rd11, %rd2;
	add.s64 	%rd12, %rd11, %rd7;
	st.global.f64 	[%rd12], %fd4;

$L__BB5_2:
	ret;

}
	// .globl	average_seq
.visible .entry average_seq(
	.param .u64 average_seq_param_0,
	.param .u64 average_seq_param_1,
	.param .u64 average_seq_param_2,
	.param .u64 average_seq_param_3,
	.param .u64 average_seq_param_4,
	.param .u64 average_seq_param_5,
	.param .u64 average_seq_param_6
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<12>;
	.reg .b64 	%rd<19>;


	ld.param.u64 	%rd2, [average_seq_param_0];
	ld.param.u64 	%rd3, [average_seq_param_1];
	ld.param.u64 	%rd7, [average_seq_param_2];
	ld.param.u64 	%rd4, [average_seq_param_3];
	ld.param.u64 	%rd5, [average_seq_param_4];
	ld.param.u64 	%rd6, [average_seq_param_5];
	ld.param.u64 	%rd8, [average_seq_param_6];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	cvt.rn.f64.s64 	%fd1, %rd8;
	setp.ge.s64 	%p1, %rd1, %rd7;
	@%p1 bra 	$L__BB6_2;

	cvta.to.global.u64 	%rd9, %rd2;
	shl.b64 	%rd10, %rd1, 3;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd2, [%rd11];
	cvta.to.global.u64 	%rd12, %rd3;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.f64 	%fd3, [%rd13];
	fma.rn.f64 	%fd4, %fd2, %fd1, %fd3;
	add.f64 	%fd5, %fd1, 0d3FF0000000000000;
	div.rn.f64 	%fd6, %fd4, %fd5;
	st.global.f64 	[%rd11], %fd6;

$L__BB6_2:
	setp.ge.s64 	%p2, %rd1, %rd6;
	@%p2 bra 	$L__BB6_4;

	cvta.to.global.u64 	%rd14, %rd4;
	shl.b64 	%rd15, %rd1, 3;
	add.s64 	%rd16, %rd14, %rd15;
	ld.global.f64 	%fd7, [%rd16];
	cvta.to.global.u64 	%rd17, %rd5;
	add.s64 	%rd18, %rd17, %rd15;
	ld.global.f64 	%fd8, [%rd18];
	fma.rn.f64 	%fd9, %fd7, %fd1, %fd8;
	add.f64 	%fd10, %fd1, 0d3FF0000000000000;
	div.rn.f64 	%fd11, %fd9, %fd10;
	st.global.f64 	[%rd16], %fd11;

$L__BB6_4:
	ret;

}
	// .globl	get_row_index
.visible .entry get_row_index(
	.param .u64 get_row_index_param_0,
	.param .u64 get_row_index_param_1,
	.param .u64 get_row_index_param_2
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<13>;
	.reg .b64 	%rd<31>;


	ld.param.u64 	%rd14, [get_row_index_param_0];
	ld.param.u64 	%rd12, [get_row_index_param_1];
	ld.param.u64 	%rd13, [get_row_index_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	cvta.to.global.u64 	%rd2, %rd14;
	shl.b64 	%rd15, %rd12, 2;
	add.s64 	%rd16, %rd2, %rd15;
	ld.global.u32 	%r5, [%rd16];
	add.s32 	%r6, %r5, -1;
	cvt.s64.s32 	%rd17, %r6;
	setp.ge.s64 	%p1, %rd1, %rd17;
	setp.lt.s64 	%p2, %rd12, 1;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB7_6;

	add.s64 	%rd29, %rd12, -1;
	cvta.to.global.u64 	%rd4, %rd13;
	mov.u64 	%rd30, 0;

$L__BB7_2:
	sub.s64 	%rd19, %rd29, %rd30;
	shr.u64 	%rd20, %rd19, 63;
	add.s64 	%rd21, %rd19, %rd20;
	shr.s64 	%rd22, %rd21, 1;
	add.s64 	%rd7, %rd22, %rd30;
	shl.b64 	%rd23, %rd7, 2;
	add.s64 	%rd8, %rd2, %rd23;
	ld.global.u32 	%r7, [%rd8];
	add.s32 	%r8, %r7, -1;
	cvt.s64.s32 	%rd9, %r8;
	setp.lt.s64 	%p4, %rd1, %rd9;
	@%p4 bra 	$L__BB7_5;

	ld.global.u32 	%r9, [%rd8+4];
	add.s32 	%r10, %r9, -1;
	cvt.s64.s32 	%rd24, %r10;
	setp.ge.s64 	%p5, %rd1, %rd24;
	@%p5 bra 	$L__BB7_5;
	bra.uni 	$L__BB7_4;

$L__BB7_5:
	add.s64 	%rd27, %rd7, 1;
	selp.b64 	%rd30, %rd30, %rd27, %p4;
	add.s64 	%rd28, %rd7, -1;
	selp.b64 	%rd29, %rd28, %rd29, %p4;
	setp.ge.s64 	%p7, %rd29, %rd30;
	@%p7 bra 	$L__BB7_2;
	bra.uni 	$L__BB7_6;

$L__BB7_4:
	cvt.u32.u64 	%r11, %rd7;
	add.s32 	%r12, %r11, 1;
	shl.b64 	%rd25, %rd1, 2;
	add.s64 	%rd26, %rd4, %rd25;
	st.global.u32 	[%rd26], %r12;

$L__BB7_6:
	ret;

}
	// .globl	rescale_coo
.visible .entry rescale_coo(
	.param .u64 rescale_coo_param_0,
	.param .u64 rescale_coo_param_1,
	.param .u64 rescale_coo_param_2,
	.param .u64 rescale_coo_param_3,
	.param .u64 rescale_coo_param_4,
	.param .u64 rescale_coo_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<22>;


	ld.param.u64 	%rd2, [rescale_coo_param_0];
	ld.param.u64 	%rd3, [rescale_coo_param_1];
	ld.param.u64 	%rd4, [rescale_coo_param_2];
	ld.param.u64 	%rd5, [rescale_coo_param_3];
	ld.param.u64 	%rd6, [rescale_coo_param_4];
	ld.param.u64 	%rd7, [rescale_coo_param_5];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd7;
	@%p1 bra 	$L__BB8_2;

	cvta.to.global.u64 	%rd8, %rd2;
	cvta.to.global.u64 	%rd9, %rd4;
	shl.b64 	%rd10, %rd1, 2;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.u32 	%r5, [%rd11];
	add.s32 	%r6, %r5, -1;
	cvta.to.global.u64 	%rd12, %rd3;
	add.s64 	%rd13, %rd12, %rd10;
	ld.global.u32 	%r7, [%rd13];
	add.s32 	%r8, %r7, -1;
	shl.b64 	%rd14, %rd1, 3;
	add.s64 	%rd15, %rd8, %rd14;
	cvta.to.global.u64 	%rd16, %rd5;
	mul.wide.s32 	%rd17, %r8, 8;
	add.s64 	%rd18, %rd16, %rd17;
	cvta.to.global.u64 	%rd19, %rd6;
	mul.wide.s32 	%rd20, %r6, 8;
	add.s64 	%rd21, %rd19, %rd20;
	ld.global.f64 	%fd1, [%rd21];
	ld.global.f64 	%fd2, [%rd18];
	mul.f64 	%fd3, %fd2, %fd1;
	ld.global.f64 	%fd4, [%rd15];
	div.rn.f64 	%fd5, %fd4, %fd3;
	st.global.f64 	[%rd15], %fd5;

$L__BB8_2:
	ret;

}
	// .globl	rescale_csr
.visible .entry rescale_csr(
	.param .u64 rescale_csr_param_0,
	.param .u64 rescale_csr_param_1,
	.param .u64 rescale_csr_param_2,
	.param .u64 rescale_csr_param_3,
	.param .u64 rescale_csr_param_4,
	.param .u64 rescale_csr_param_5,
	.param .u64 rescale_csr_param_6
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<13>;
	.reg .f64 	%fd<6>;
	.reg .b64 	%rd<47>;


	ld.param.u64 	%rd12, [rescale_csr_param_0];
	ld.param.u64 	%rd17, [rescale_csr_param_1];
	ld.param.u64 	%rd13, [rescale_csr_param_2];
	ld.param.u64 	%rd14, [rescale_csr_param_3];
	ld.param.u64 	%rd15, [rescale_csr_param_4];
	ld.param.u64 	%rd16, [rescale_csr_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r2, %r1, %r3;
	cvt.u64.u32 	%rd1, %r4;
	cvta.to.global.u64 	%rd2, %rd17;
	shl.b64 	%rd18, %rd16, 2;
	add.s64 	%rd19, %rd2, %rd18;
	ld.global.u32 	%r5, [%rd19];
	add.s32 	%r6, %r5, -1;
	cvt.s64.s32 	%rd20, %r6;
	setp.ge.s64 	%p1, %rd1, %rd20;
	@%p1 bra 	$L__BB9_7;

	setp.lt.s64 	%p2, %rd16, 1;
	mov.u64 	%rd46, 0;
	@%p2 bra 	$L__BB9_6;

	add.s64 	%rd44, %rd16, -1;
	mov.u64 	%rd45, 0;

$L__BB9_3:
	sub.s64 	%rd23, %rd44, %rd45;
	shr.u64 	%rd24, %rd23, 63;
	add.s64 	%rd25, %rd23, %rd24;
	shr.s64 	%rd26, %rd25, 1;
	add.s64 	%rd46, %rd26, %rd45;
	shl.b64 	%rd27, %rd46, 2;
	add.s64 	%rd7, %rd2, %rd27;
	ld.global.u32 	%r7, [%rd7];
	add.s32 	%r8, %r7, -1;
	cvt.s64.s32 	%rd8, %r8;
	setp.lt.s64 	%p3, %rd1, %rd8;
	@%p3 bra 	$L__BB9_5;

	ld.global.u32 	%r9, [%rd7+4];
	add.s32 	%r10, %r9, -1;
	cvt.s64.s32 	%rd28, %r10;
	setp.lt.s64 	%p4, %rd1, %rd28;
	@%p4 bra 	$L__BB9_6;

$L__BB9_5:
	add.s64 	%rd30, %rd46, 1;
	selp.b64 	%rd45, %rd45, %rd30, %p3;
	add.s64 	%rd31, %rd46, -1;
	selp.b64 	%rd44, %rd31, %rd44, %p3;
	setp.ge.s64 	%p6, %rd44, %rd45;
	mov.u64 	%rd46, 0;
	@%p6 bra 	$L__BB9_3;

$L__BB9_6:
	cvta.to.global.u64 	%rd32, %rd13;
	shl.b64 	%rd33, %rd1, 2;
	add.s64 	%rd34, %rd32, %rd33;
	ld.global.u32 	%r11, [%rd34];
	add.s32 	%r12, %r11, -1;
	cvta.to.global.u64 	%rd35, %rd15;
	mul.wide.s32 	%rd36, %r12, 8;
	add.s64 	%rd37, %rd35, %rd36;
	ld.global.f64 	%fd1, [%rd37];
	cvta.to.global.u64 	%rd38, %rd14;
	shl.b64 	%rd39, %rd46, 3;
	add.s64 	%rd40, %rd38, %rd39;
	ld.global.f64 	%fd2, [%rd40];
	mul.f64 	%fd3, %fd2, %fd1;
	cvta.to.global.u64 	%rd41, %rd12;
	shl.b64 	%rd42, %rd1, 3;
	add.s64 	%rd43, %rd41, %rd42;
	ld.global.f64 	%fd4, [%rd43];
	div.rn.f64 	%fd5, %fd4, %fd3;
	st.global.f64 	[%rd43], %fd5;

$L__BB9_7:
	ret;

}
	// .globl	replace_inf_with_zero
.visible .entry replace_inf_with_zero(
	.param .u64 replace_inf_with_zero_param_0,
	.param .u64 replace_inf_with_zero_param_1,
	.param .u64 replace_inf_with_zero_param_2
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<5>;
	.reg .f64 	%fd<5>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd2, [replace_inf_with_zero_param_0];
	ld.param.u64 	%rd3, [replace_inf_with_zero_param_1];
	ld.param.u64 	%rd4, [replace_inf_with_zero_param_2];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd4;
	@%p1 bra 	$L__BB10_2;

	cvta.to.global.u64 	%rd5, %rd3;
	cvta.to.global.u64 	%rd6, %rd2;
	shl.b64 	%rd7, %rd1, 3;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	setp.eq.f64 	%p2, %fd1, 0dFFF0000000000000;
	selp.f64 	%fd2, 0d0000000000000000, %fd1, %p2;
	st.global.f64 	[%rd8], %fd2;
	add.s64 	%rd9, %rd5, %rd7;
	ld.global.f64 	%fd3, [%rd9];
	setp.eq.f64 	%p3, %fd3, 0d7FF0000000000000;
	selp.f64 	%fd4, 0d0000000000000000, %fd3, %p3;
	st.global.f64 	[%rd9], %fd4;

$L__BB10_2:
	ret;

}
	// .globl	max_abs_row_elementwise_kernel
.visible .entry max_abs_row_elementwise_kernel(
	.param .u64 max_abs_row_elementwise_kernel_param_0,
	.param .u64 max_abs_row_elementwise_kernel_param_1,
	.param .u64 max_abs_row_elementwise_kernel_param_2,
	.param .u64 max_abs_row_elementwise_kernel_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd2, [max_abs_row_elementwise_kernel_param_0];
	ld.param.u64 	%rd3, [max_abs_row_elementwise_kernel_param_1];
	ld.param.u64 	%rd5, [max_abs_row_elementwise_kernel_param_2];
	ld.param.u64 	%rd4, [max_abs_row_elementwise_kernel_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd5;
	@%p1 bra 	$L__BB11_2;

	cvta.to.global.u64 	%rd6, %rd3;
	shl.b64 	%rd7, %rd1, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r5, [%rd8];
	add.s32 	%r6, %r5, -1;
	cvta.to.global.u64 	%rd9, %rd2;
	shl.b64 	%rd10, %rd1, 3;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r6, 8;
	add.s64 	%rd14, %rd12, %rd13;
	mov.b64 	%rd15, %fd2;
	atom.global.max.u64 	%rd16, [%rd14], %rd15;

$L__BB11_2:
	ret;

}
	// .globl	max_abs_row_kernel
.visible .entry max_abs_row_kernel(
	.param .u64 max_abs_row_kernel_param_0,
	.param .u64 max_abs_row_kernel_param_1,
	.param .u64 max_abs_row_kernel_param_2,
	.param .u64 max_abs_row_kernel_param_3
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<76>;


	ld.param.u64 	%rd39, [max_abs_row_kernel_param_0];
	ld.param.u64 	%rd36, [max_abs_row_kernel_param_1];
	ld.param.u64 	%rd37, [max_abs_row_kernel_param_2];
	ld.param.u64 	%rd38, [max_abs_row_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd39;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r2, %r4;
	cvt.u64.u32 	%rd2, %r5;
	mov.u32 	%r6, WARP_SZ;
	cvt.s64.s32 	%rd3, %r6;
	and.b64  	%rd40, %rd3, -4294967296;
	setp.eq.s64 	%p1, %rd40, 0;
	@%p1 bra 	$L__BB12_2;

	div.s64 	%rd66, %rd2, %rd3;
	mul.lo.s64 	%rd41, %rd66, %rd3;
	sub.s64 	%rd67, %rd2, %rd41;
	bra.uni 	$L__BB12_3;

$L__BB12_2:
	cvt.u32.u64 	%r7, %rd3;
	cvt.u32.u64 	%r8, %rd2;
	div.u32 	%r9, %r8, %r7;
	mul.lo.s32 	%r10, %r9, %r7;
	sub.s32 	%r11, %r8, %r10;
	cvt.u64.u32 	%rd66, %r9;
	cvt.u64.u32 	%rd67, %r11;

$L__BB12_3:
	setp.ge.s64 	%p2, %rd66, %rd37;
	@%p2 bra 	$L__BB12_19;

	cvta.to.global.u64 	%rd42, %rd36;
	shl.b64 	%rd43, %rd66, 2;
	add.s64 	%rd44, %rd42, %rd43;
	ld.global.u32 	%r1, [%rd44];
	add.s32 	%r12, %r1, -1;
	cvt.s64.s32 	%rd45, %r12;
	ld.global.u32 	%r13, [%rd44+4];
	add.s32 	%r14, %r13, -1;
	cvt.s64.s32 	%rd10, %r14;
	add.s64 	%rd72, %rd67, %rd45;
	setp.ge.s64 	%p3, %rd72, %rd10;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p3 bra 	$L__BB12_14;

	not.b64 	%rd46, %rd72;
	add.s64 	%rd12, %rd46, %rd10;
	or.b64  	%rd47, %rd12, %rd3;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB12_7;

	div.u64 	%rd68, %rd12, %rd3;
	bra.uni 	$L__BB12_8;

$L__BB12_7:
	cvt.u32.u64 	%r15, %rd3;
	cvt.u32.u64 	%r16, %rd12;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd68, %r17;

$L__BB12_8:
	add.s64 	%rd49, %rd68, 1;
	and.b64  	%rd71, %rd49, 3;
	setp.eq.s64 	%p5, %rd71, 0;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p5 bra 	$L__BB12_11;

	cvt.s64.s32 	%rd50, %r1;
	add.s64 	%rd51, %rd67, %rd50;
	add.s64 	%rd72, %rd51, -1;
	shl.b64 	%rd52, %rd51, 3;
	add.s64 	%rd53, %rd1, %rd52;
	add.s64 	%rd69, %rd53, -8;
	shl.b64 	%rd19, %rd3, 3;

$L__BB12_10:
	.pragma "nounroll";
	ld.global.f64 	%fd15, [%rd69];
	abs.f64 	%fd16, %fd15;
	max.f64 	%fd34, %fd34, %fd16;
	add.s64 	%rd72, %rd72, %rd3;
	add.s64 	%rd69, %rd69, %rd19;
	add.s64 	%rd71, %rd71, -1;
	setp.ne.s64 	%p6, %rd71, 0;
	@%p6 bra 	$L__BB12_10;

$L__BB12_11:
	setp.lt.u64 	%p7, %rd68, 3;
	@%p7 bra 	$L__BB12_14;

	shl.b64 	%rd54, %rd72, 3;
	add.s64 	%rd74, %rd1, %rd54;
	shl.b64 	%rd28, %rd3, 3;

$L__BB12_13:
	ld.global.f64 	%fd17, [%rd74];
	abs.f64 	%fd18, %fd17;
	max.f64 	%fd19, %fd34, %fd18;
	add.s64 	%rd55, %rd74, %rd28;
	ld.global.f64 	%fd20, [%rd55];
	abs.f64 	%fd21, %fd20;
	max.f64 	%fd22, %fd19, %fd21;
	add.s64 	%rd56, %rd72, %rd3;
	add.s64 	%rd57, %rd56, %rd3;
	add.s64 	%rd58, %rd55, %rd28;
	ld.global.f64 	%fd23, [%rd58];
	abs.f64 	%fd24, %fd23;
	max.f64 	%fd25, %fd22, %fd24;
	add.s64 	%rd59, %rd57, %rd3;
	add.s64 	%rd60, %rd58, %rd28;
	add.s64 	%rd74, %rd60, %rd28;
	ld.global.f64 	%fd26, [%rd60];
	abs.f64 	%fd27, %fd26;
	max.f64 	%fd34, %fd25, %fd27;
	add.s64 	%rd72, %rd59, %rd3;
	setp.lt.s64 	%p8, %rd72, %rd10;
	@%p8 bra 	$L__BB12_13;

$L__BB12_14:
	cvt.u32.u64 	%r18, %rd3;
	setp.lt.s32 	%p9, %r18, 2;
	@%p9 bra 	$L__BB12_17;

	shr.u32 	%r20, %r18, 31;
	add.s32 	%r21, %r18, %r20;
	shr.s32 	%r22, %r21, 1;
	cvt.s64.s32 	%rd75, %r22;

$L__BB12_16:
	// begin inline asm
	mov.b64 {%r23,%r24}, %fd34;
	// end inline asm
	cvt.u32.u64 	%r27, %rd75;
	mov.u32 	%r28, 31;
	mov.u32 	%r29, -1;
	shfl.sync.down.b32 	%r26|%p10, %r24, %r27, %r28, %r29;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r27, %r28, %r29;
	// begin inline asm
	mov.b64 %fd29, {%r25,%r26};
	// end inline asm
	max.f64 	%fd34, %fd34, %fd29;
	shr.u64 	%rd61, %rd75, 63;
	add.s64 	%rd62, %rd75, %rd61;
	shr.s64 	%rd35, %rd62, 1;
	setp.gt.s64 	%p12, %rd75, 1;
	mov.u64 	%rd75, %rd35;
	@%p12 bra 	$L__BB12_16;

$L__BB12_17:
	setp.ne.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB12_19;

	cvta.to.global.u64 	%rd63, %rd38;
	shl.b64 	%rd64, %rd66, 3;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f64 	[%rd65], %fd34;

$L__BB12_19:
	ret;

}
	// .globl	max_abs_col_elementwise_kernel
.visible .entry max_abs_col_elementwise_kernel(
	.param .u64 max_abs_col_elementwise_kernel_param_0,
	.param .u64 max_abs_col_elementwise_kernel_param_1,
	.param .u64 max_abs_col_elementwise_kernel_param_2,
	.param .u64 max_abs_col_elementwise_kernel_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd2, [max_abs_col_elementwise_kernel_param_0];
	ld.param.u64 	%rd3, [max_abs_col_elementwise_kernel_param_1];
	ld.param.u64 	%rd5, [max_abs_col_elementwise_kernel_param_2];
	ld.param.u64 	%rd4, [max_abs_col_elementwise_kernel_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd5;
	@%p1 bra 	$L__BB13_2;

	cvta.to.global.u64 	%rd6, %rd3;
	shl.b64 	%rd7, %rd1, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r5, [%rd8];
	add.s32 	%r6, %r5, -1;
	cvta.to.global.u64 	%rd9, %rd2;
	shl.b64 	%rd10, %rd1, 3;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r6, 8;
	add.s64 	%rd14, %rd12, %rd13;
	mov.b64 	%rd15, %fd2;
	atom.global.max.u64 	%rd16, [%rd14], %rd15;

$L__BB13_2:
	ret;

}
	// .globl	max_abs_col_kernel
.visible .entry max_abs_col_kernel(
	.param .u64 max_abs_col_kernel_param_0,
	.param .u64 max_abs_col_kernel_param_1,
	.param .u64 max_abs_col_kernel_param_2,
	.param .u64 max_abs_col_kernel_param_3,
	.param .u64 max_abs_col_kernel_param_4,
	.param .u64 max_abs_col_kernel_param_5
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd25, [max_abs_col_kernel_param_0];
	ld.param.u64 	%rd26, [max_abs_col_kernel_param_1];
	ld.param.u64 	%rd27, [max_abs_col_kernel_param_2];
	ld.param.u64 	%rd28, [max_abs_col_kernel_param_3];
	ld.param.u64 	%rd29, [max_abs_col_kernel_param_4];
	ld.param.u64 	%rd30, [max_abs_col_kernel_param_5];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r4, %r3, %r5;
	cvt.u64.u32 	%rd1, %r6;
	mov.u32 	%r7, WARP_SZ;
	cvt.s64.s32 	%rd2, %r7;
	and.b64  	%rd31, %rd2, -4294967296;
	setp.eq.s64 	%p1, %rd31, 0;
	@%p1 bra 	$L__BB14_2;

	div.s64 	%rd50, %rd1, %rd2;
	mul.lo.s64 	%rd32, %rd50, %rd2;
	sub.s64 	%rd51, %rd1, %rd32;
	bra.uni 	$L__BB14_3;

$L__BB14_2:
	cvt.u32.u64 	%r8, %rd2;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd50, %r10;
	cvt.u64.u32 	%rd51, %r12;

$L__BB14_3:
	setp.ge.s64 	%p2, %rd50, %rd29;
	@%p2 bra 	$L__BB14_17;

	setp.ge.s64 	%p3, %rd51, %rd28;
	mov.f64 	%fd14, 0d0000000000000000;
	@%p3 bra 	$L__BB14_12;

	cvta.to.global.u64 	%rd9, %rd27;
	cvta.to.global.u64 	%rd10, %rd26;
	cvta.to.global.u64 	%rd11, %rd25;
	mov.u64 	%rd52, %rd51;

$L__BB14_6:
	shl.b64 	%rd33, %rd52, 2;
	add.s64 	%rd34, %rd9, %rd33;
	ld.global.u32 	%r13, [%rd34];
	add.s32 	%r1, %r13, -1;
	ld.global.u32 	%r14, [%rd34+4];
	add.s32 	%r2, %r14, -2;
	setp.lt.s32 	%p4, %r2, %r1;
	@%p4 bra 	$L__BB14_11;

	cvt.s64.s32 	%rd53, %r2;
	cvt.s64.s32 	%rd54, %r1;

$L__BB14_8:
	sub.s64 	%rd35, %rd53, %rd54;
	shr.u64 	%rd36, %rd35, 63;
	add.s64 	%rd37, %rd35, %rd36;
	shr.s64 	%rd38, %rd37, 1;
	add.s64 	%rd17, %rd38, %rd54;
	shl.b64 	%rd39, %rd17, 2;
	add.s64 	%rd40, %rd10, %rd39;
	ld.global.u32 	%r15, [%rd40];
	add.s32 	%r16, %r15, -1;
	cvt.s64.s32 	%rd18, %r16;
	setp.eq.s64 	%p5, %rd50, %rd18;
	@%p5 bra 	$L__BB14_10;

	setp.lt.s64 	%p6, %rd50, %rd18;
	add.s64 	%rd41, %rd17, 1;
	selp.b64 	%rd54, %rd54, %rd41, %p6;
	add.s64 	%rd42, %rd17, -1;
	selp.b64 	%rd53, %rd42, %rd53, %p6;
	setp.lt.s64 	%p7, %rd53, %rd54;
	@%p7 bra 	$L__BB14_11;
	bra.uni 	$L__BB14_8;

$L__BB14_10:
	shl.b64 	%rd43, %rd17, 3;
	add.s64 	%rd44, %rd11, %rd43;
	ld.global.f64 	%fd10, [%rd44];
	abs.f64 	%fd11, %fd10;
	max.f64 	%fd14, %fd14, %fd11;

$L__BB14_11:
	add.s64 	%rd52, %rd52, %rd2;
	setp.lt.s64 	%p8, %rd52, %rd28;
	@%p8 bra 	$L__BB14_6;

$L__BB14_12:
	cvt.u32.u64 	%r17, %rd2;
	setp.lt.s32 	%p9, %r17, 2;
	@%p9 bra 	$L__BB14_15;

	shr.u32 	%r19, %r17, 31;
	add.s32 	%r20, %r17, %r19;
	shr.s32 	%r21, %r20, 1;
	cvt.s64.s32 	%rd55, %r21;

$L__BB14_14:
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd14;
	// end inline asm
	cvt.u32.u64 	%r26, %rd55;
	mov.u32 	%r27, 31;
	mov.u32 	%r28, -1;
	shfl.sync.down.b32 	%r25|%p10, %r23, %r26, %r27, %r28;
	shfl.sync.down.b32 	%r24|%p11, %r22, %r26, %r27, %r28;
	// begin inline asm
	mov.b64 %fd13, {%r24,%r25};
	// end inline asm
	max.f64 	%fd14, %fd14, %fd13;
	shr.u64 	%rd45, %rd55, 63;
	add.s64 	%rd46, %rd55, %rd45;
	shr.s64 	%rd24, %rd46, 1;
	setp.gt.s64 	%p12, %rd55, 1;
	mov.u64 	%rd55, %rd24;
	@%p12 bra 	$L__BB14_14;

$L__BB14_15:
	setp.ne.s64 	%p13, %rd51, 0;
	@%p13 bra 	$L__BB14_17;

	cvta.to.global.u64 	%rd47, %rd30;
	shl.b64 	%rd48, %rd50, 3;
	add.s64 	%rd49, %rd47, %rd48;
	st.global.f64 	[%rd49], %fd14;

$L__BB14_17:
	ret;

}
	// .globl	alpha_norm_row_kernel
.visible .entry alpha_norm_row_kernel(
	.param .u64 alpha_norm_row_kernel_param_0,
	.param .u64 alpha_norm_row_kernel_param_1,
	.param .u64 alpha_norm_row_kernel_param_2,
	.param .u64 alpha_norm_row_kernel_param_3,
	.param .f64 alpha_norm_row_kernel_param_4
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<30>;
	.reg .f64 	%fd<37>;
	.reg .b64 	%rd<76>;


	ld.param.u64 	%rd39, [alpha_norm_row_kernel_param_0];
	ld.param.u64 	%rd36, [alpha_norm_row_kernel_param_1];
	ld.param.u64 	%rd37, [alpha_norm_row_kernel_param_2];
	ld.param.u64 	%rd38, [alpha_norm_row_kernel_param_3];
	cvta.to.global.u64 	%rd1, %rd39;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %ctaid.x;
	mov.u32 	%r4, %tid.x;
	mad.lo.s32 	%r5, %r3, %r2, %r4;
	cvt.u64.u32 	%rd2, %r5;
	mov.u32 	%r6, WARP_SZ;
	cvt.s64.s32 	%rd3, %r6;
	and.b64  	%rd40, %rd3, -4294967296;
	setp.eq.s64 	%p1, %rd40, 0;
	@%p1 bra 	$L__BB15_2;

	div.s64 	%rd66, %rd2, %rd3;
	mul.lo.s64 	%rd41, %rd66, %rd3;
	sub.s64 	%rd67, %rd2, %rd41;
	bra.uni 	$L__BB15_3;

$L__BB15_2:
	cvt.u32.u64 	%r7, %rd3;
	cvt.u32.u64 	%r8, %rd2;
	div.u32 	%r9, %r8, %r7;
	mul.lo.s32 	%r10, %r9, %r7;
	sub.s32 	%r11, %r8, %r10;
	cvt.u64.u32 	%rd66, %r9;
	cvt.u64.u32 	%rd67, %r11;

$L__BB15_3:
	setp.ge.s64 	%p2, %rd66, %rd37;
	@%p2 bra 	$L__BB15_19;

	cvta.to.global.u64 	%rd42, %rd36;
	shl.b64 	%rd43, %rd66, 2;
	add.s64 	%rd44, %rd42, %rd43;
	ld.global.u32 	%r1, [%rd44];
	add.s32 	%r12, %r1, -1;
	cvt.s64.s32 	%rd45, %r12;
	ld.global.u32 	%r13, [%rd44+4];
	add.s32 	%r14, %r13, -1;
	cvt.s64.s32 	%rd10, %r14;
	add.s64 	%rd72, %rd67, %rd45;
	setp.ge.s64 	%p3, %rd72, %rd10;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p3 bra 	$L__BB15_14;

	not.b64 	%rd46, %rd72;
	add.s64 	%rd12, %rd46, %rd10;
	or.b64  	%rd47, %rd12, %rd3;
	and.b64  	%rd48, %rd47, -4294967296;
	setp.eq.s64 	%p4, %rd48, 0;
	@%p4 bra 	$L__BB15_7;

	div.u64 	%rd68, %rd12, %rd3;
	bra.uni 	$L__BB15_8;

$L__BB15_7:
	cvt.u32.u64 	%r15, %rd3;
	cvt.u32.u64 	%r16, %rd12;
	div.u32 	%r17, %r16, %r15;
	cvt.u64.u32 	%rd68, %r17;

$L__BB15_8:
	add.s64 	%rd49, %rd68, 1;
	and.b64  	%rd71, %rd49, 3;
	setp.eq.s64 	%p5, %rd71, 0;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p5 bra 	$L__BB15_11;

	cvt.s64.s32 	%rd50, %r1;
	add.s64 	%rd51, %rd67, %rd50;
	add.s64 	%rd72, %rd51, -1;
	shl.b64 	%rd52, %rd51, 3;
	add.s64 	%rd53, %rd1, %rd52;
	add.s64 	%rd69, %rd53, -8;
	shl.b64 	%rd19, %rd3, 3;

$L__BB15_10:
	.pragma "nounroll";
	ld.global.f64 	%fd15, [%rd69];
	abs.f64 	%fd16, %fd15;
	add.f64 	%fd34, %fd34, %fd16;
	add.s64 	%rd72, %rd72, %rd3;
	add.s64 	%rd69, %rd69, %rd19;
	add.s64 	%rd71, %rd71, -1;
	setp.ne.s64 	%p6, %rd71, 0;
	@%p6 bra 	$L__BB15_10;

$L__BB15_11:
	setp.lt.u64 	%p7, %rd68, 3;
	@%p7 bra 	$L__BB15_14;

	shl.b64 	%rd54, %rd72, 3;
	add.s64 	%rd74, %rd1, %rd54;
	shl.b64 	%rd28, %rd3, 3;

$L__BB15_13:
	ld.global.f64 	%fd17, [%rd74];
	abs.f64 	%fd18, %fd17;
	add.f64 	%fd19, %fd34, %fd18;
	add.s64 	%rd55, %rd74, %rd28;
	ld.global.f64 	%fd20, [%rd55];
	abs.f64 	%fd21, %fd20;
	add.f64 	%fd22, %fd19, %fd21;
	add.s64 	%rd56, %rd72, %rd3;
	add.s64 	%rd57, %rd56, %rd3;
	add.s64 	%rd58, %rd55, %rd28;
	ld.global.f64 	%fd23, [%rd58];
	abs.f64 	%fd24, %fd23;
	add.f64 	%fd25, %fd22, %fd24;
	add.s64 	%rd59, %rd57, %rd3;
	add.s64 	%rd60, %rd58, %rd28;
	add.s64 	%rd74, %rd60, %rd28;
	ld.global.f64 	%fd26, [%rd60];
	abs.f64 	%fd27, %fd26;
	add.f64 	%fd34, %fd25, %fd27;
	add.s64 	%rd72, %rd59, %rd3;
	setp.lt.s64 	%p8, %rd72, %rd10;
	@%p8 bra 	$L__BB15_13;

$L__BB15_14:
	cvt.u32.u64 	%r18, %rd3;
	setp.lt.s32 	%p9, %r18, 2;
	@%p9 bra 	$L__BB15_17;

	shr.u32 	%r20, %r18, 31;
	add.s32 	%r21, %r18, %r20;
	shr.s32 	%r22, %r21, 1;
	cvt.s64.s32 	%rd75, %r22;

$L__BB15_16:
	// begin inline asm
	mov.b64 {%r23,%r24}, %fd34;
	// end inline asm
	cvt.u32.u64 	%r27, %rd75;
	mov.u32 	%r28, 31;
	mov.u32 	%r29, -1;
	shfl.sync.down.b32 	%r26|%p10, %r24, %r27, %r28, %r29;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r27, %r28, %r29;
	// begin inline asm
	mov.b64 %fd29, {%r25,%r26};
	// end inline asm
	add.f64 	%fd34, %fd34, %fd29;
	shr.u64 	%rd61, %rd75, 63;
	add.s64 	%rd62, %rd75, %rd61;
	shr.s64 	%rd35, %rd62, 1;
	setp.gt.s64 	%p12, %rd75, 1;
	mov.u64 	%rd75, %rd35;
	@%p12 bra 	$L__BB15_16;

$L__BB15_17:
	setp.ne.s64 	%p13, %rd67, 0;
	@%p13 bra 	$L__BB15_19;

	cvta.to.global.u64 	%rd63, %rd38;
	shl.b64 	%rd64, %rd66, 3;
	add.s64 	%rd65, %rd63, %rd64;
	st.global.f64 	[%rd65], %fd34;

$L__BB15_19:
	ret;

}
	// .globl	alpha_norm_col_kernel
.visible .entry alpha_norm_col_kernel(
	.param .u64 alpha_norm_col_kernel_param_0,
	.param .u64 alpha_norm_col_kernel_param_1,
	.param .u64 alpha_norm_col_kernel_param_2,
	.param .u64 alpha_norm_col_kernel_param_3,
	.param .u64 alpha_norm_col_kernel_param_4,
	.param .u64 alpha_norm_col_kernel_param_5,
	.param .f64 alpha_norm_col_kernel_param_6
)
{
	.reg .pred 	%p<14>;
	.reg .b32 	%r<29>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<56>;


	ld.param.u64 	%rd25, [alpha_norm_col_kernel_param_0];
	ld.param.u64 	%rd26, [alpha_norm_col_kernel_param_1];
	ld.param.u64 	%rd27, [alpha_norm_col_kernel_param_2];
	ld.param.u64 	%rd28, [alpha_norm_col_kernel_param_3];
	ld.param.u64 	%rd29, [alpha_norm_col_kernel_param_4];
	ld.param.u64 	%rd30, [alpha_norm_col_kernel_param_5];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r6, %r4, %r3, %r5;
	cvt.u64.u32 	%rd1, %r6;
	mov.u32 	%r7, WARP_SZ;
	cvt.s64.s32 	%rd2, %r7;
	and.b64  	%rd31, %rd2, -4294967296;
	setp.eq.s64 	%p1, %rd31, 0;
	@%p1 bra 	$L__BB16_2;

	div.s64 	%rd50, %rd1, %rd2;
	mul.lo.s64 	%rd32, %rd50, %rd2;
	sub.s64 	%rd51, %rd1, %rd32;
	bra.uni 	$L__BB16_3;

$L__BB16_2:
	cvt.u32.u64 	%r8, %rd2;
	cvt.u32.u64 	%r9, %rd1;
	div.u32 	%r10, %r9, %r8;
	mul.lo.s32 	%r11, %r10, %r8;
	sub.s32 	%r12, %r9, %r11;
	cvt.u64.u32 	%rd50, %r10;
	cvt.u64.u32 	%rd51, %r12;

$L__BB16_3:
	setp.ge.s64 	%p2, %rd50, %rd29;
	@%p2 bra 	$L__BB16_17;

	setp.ge.s64 	%p3, %rd51, %rd28;
	mov.f64 	%fd14, 0d0000000000000000;
	@%p3 bra 	$L__BB16_12;

	cvta.to.global.u64 	%rd9, %rd27;
	cvta.to.global.u64 	%rd10, %rd26;
	cvta.to.global.u64 	%rd11, %rd25;
	mov.u64 	%rd52, %rd51;

$L__BB16_6:
	shl.b64 	%rd33, %rd52, 2;
	add.s64 	%rd34, %rd9, %rd33;
	ld.global.u32 	%r13, [%rd34];
	add.s32 	%r1, %r13, -1;
	ld.global.u32 	%r14, [%rd34+4];
	add.s32 	%r2, %r14, -2;
	setp.lt.s32 	%p4, %r2, %r1;
	@%p4 bra 	$L__BB16_11;

	cvt.s64.s32 	%rd53, %r2;
	cvt.s64.s32 	%rd54, %r1;

$L__BB16_8:
	sub.s64 	%rd35, %rd53, %rd54;
	shr.u64 	%rd36, %rd35, 63;
	add.s64 	%rd37, %rd35, %rd36;
	shr.s64 	%rd38, %rd37, 1;
	add.s64 	%rd17, %rd38, %rd54;
	shl.b64 	%rd39, %rd17, 2;
	add.s64 	%rd40, %rd10, %rd39;
	ld.global.u32 	%r15, [%rd40];
	add.s32 	%r16, %r15, -1;
	cvt.s64.s32 	%rd18, %r16;
	setp.eq.s64 	%p5, %rd50, %rd18;
	@%p5 bra 	$L__BB16_10;

	setp.lt.s64 	%p6, %rd50, %rd18;
	add.s64 	%rd41, %rd17, 1;
	selp.b64 	%rd54, %rd54, %rd41, %p6;
	add.s64 	%rd42, %rd17, -1;
	selp.b64 	%rd53, %rd42, %rd53, %p6;
	setp.lt.s64 	%p7, %rd53, %rd54;
	@%p7 bra 	$L__BB16_11;
	bra.uni 	$L__BB16_8;

$L__BB16_10:
	shl.b64 	%rd43, %rd17, 3;
	add.s64 	%rd44, %rd11, %rd43;
	ld.global.f64 	%fd10, [%rd44];
	abs.f64 	%fd11, %fd10;
	add.f64 	%fd14, %fd14, %fd11;

$L__BB16_11:
	add.s64 	%rd52, %rd52, %rd2;
	setp.lt.s64 	%p8, %rd52, %rd28;
	@%p8 bra 	$L__BB16_6;

$L__BB16_12:
	cvt.u32.u64 	%r17, %rd2;
	setp.lt.s32 	%p9, %r17, 2;
	@%p9 bra 	$L__BB16_15;

	shr.u32 	%r19, %r17, 31;
	add.s32 	%r20, %r17, %r19;
	shr.s32 	%r21, %r20, 1;
	cvt.s64.s32 	%rd55, %r21;

$L__BB16_14:
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd14;
	// end inline asm
	cvt.u32.u64 	%r26, %rd55;
	mov.u32 	%r27, 31;
	mov.u32 	%r28, -1;
	shfl.sync.down.b32 	%r25|%p10, %r23, %r26, %r27, %r28;
	shfl.sync.down.b32 	%r24|%p11, %r22, %r26, %r27, %r28;
	// begin inline asm
	mov.b64 %fd13, {%r24,%r25};
	// end inline asm
	add.f64 	%fd14, %fd14, %fd13;
	shr.u64 	%rd45, %rd55, 63;
	add.s64 	%rd46, %rd55, %rd45;
	shr.s64 	%rd24, %rd46, 1;
	setp.gt.s64 	%p12, %rd55, 1;
	mov.u64 	%rd55, %rd24;
	@%p12 bra 	$L__BB16_14;

$L__BB16_15:
	setp.ne.s64 	%p13, %rd51, 0;
	@%p13 bra 	$L__BB16_17;

	cvta.to.global.u64 	%rd47, %rd30;
	shl.b64 	%rd48, %rd50, 3;
	add.s64 	%rd49, %rd47, %rd48;
	st.global.f64 	[%rd49], %fd14;

$L__BB16_17:
	ret;

}
	// .globl	alpha_norm_col_elementwise_kernel
.visible .entry alpha_norm_col_elementwise_kernel(
	.param .u64 alpha_norm_col_elementwise_kernel_param_0,
	.param .u64 alpha_norm_col_elementwise_kernel_param_1,
	.param .u64 alpha_norm_col_elementwise_kernel_param_2,
	.param .u64 alpha_norm_col_elementwise_kernel_param_3,
	.param .f64 alpha_norm_col_elementwise_kernel_param_4,
	.param .u32 alpha_norm_col_elementwise_kernel_param_5
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<7>;
	.reg .f64 	%fd<4>;
	.reg .b64 	%rd<15>;


	ld.param.u64 	%rd2, [alpha_norm_col_elementwise_kernel_param_0];
	ld.param.u64 	%rd3, [alpha_norm_col_elementwise_kernel_param_1];
	ld.param.u64 	%rd5, [alpha_norm_col_elementwise_kernel_param_2];
	ld.param.u64 	%rd4, [alpha_norm_col_elementwise_kernel_param_3];
	mov.u32 	%r1, %ctaid.x;
	mov.u32 	%r2, %ntid.x;
	mov.u32 	%r3, %tid.x;
	mad.lo.s32 	%r4, %r1, %r2, %r3;
	cvt.u64.u32 	%rd1, %r4;
	setp.ge.s64 	%p1, %rd1, %rd5;
	@%p1 bra 	$L__BB17_2;

	cvta.to.global.u64 	%rd6, %rd3;
	shl.b64 	%rd7, %rd1, 2;
	add.s64 	%rd8, %rd6, %rd7;
	ld.global.u32 	%r5, [%rd8];
	add.s32 	%r6, %r5, -1;
	cvta.to.global.u64 	%rd9, %rd2;
	shl.b64 	%rd10, %rd1, 3;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.f64 	%fd1, [%rd11];
	abs.f64 	%fd2, %fd1;
	cvta.to.global.u64 	%rd12, %rd4;
	mul.wide.s32 	%rd13, %r6, 8;
	add.s64 	%rd14, %rd12, %rd13;
	atom.global.add.f64 	%fd3, [%rd14], %fd2;

$L__BB17_2:
	ret;

}

